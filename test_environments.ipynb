{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMF1SlN0Aj7cGoGNvc/sNs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhazeRoOman/DeepDTA/blob/Menna's-branch/test_environments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9hycfGT26LV",
        "outputId": "8dcd7847-ffc5-4bba-fdb3-635e675fd0c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wuyf_bHUltb_",
        "outputId": "81a4d80d-7f56-42a6-8414-52a12e7aba2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtHmbkwyWs3u",
        "outputId": "6c4286fa-57aa-42da-9184-23285385c7a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [457 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,103 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,235 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.9 kB]\n",
            "Fetched 3,188 kB in 5s (637 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Menna44/DeepDTA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxTmjKyV2p3V",
        "outputId": "51dcab98-8ac5-4d24-dd45-4b68301e9a08"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepDTA' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepDTA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6krld5726T9",
        "outputId": "b18004a0-1cf2-4006-e752-5cc8f54631c0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepDTA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TzrfO9vkaDx",
        "outputId": "06059d84-3424-46b2-8652-ccf8cfacd898"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepDTA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports:**"
      ],
      "metadata": {
        "id": "qZCtWN_h8nGG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rn9WsQM18q-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "#import matplotlib\n",
        "#matplotlib.use('Agg')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "### We modified Pahikkala et al. (2014) source code for cross-val process ###\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "np.random.seed(1)\n",
        "rn.seed(1)"
      ],
      "metadata": {
        "id": "YOdeUf5W5kw1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n"
      ],
      "metadata": {
        "id": "1Fyv7a1W5pmR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datahelper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxjdJxrr60eP",
        "outputId": "7b48d683-3763-45a8-fce3-012bf1912dbf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datahelper\n",
            "  Downloading datahelper-0.0.4-py3-none-any.whl (9.2 kB)\n",
            "Installing collected packages: datahelper\n",
            "Successfully installed datahelper-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3EGT3Z58b3l",
        "outputId": "6caed7d8-5da2-4f46-d172-db8f04078ac1"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m868.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.25.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Installing collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from datahelper import *\n",
        "import sys\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot\n",
        "import json\n",
        "import pickle\n",
        "import collections\n",
        "from collections import OrderedDict\n",
        "from matplotlib.pyplot import cm\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "## ######################## ##\n",
        "#\n",
        "#  Define CHARSET, CHARLEN\n",
        "#\n",
        "## ######################## ##\n",
        "\n",
        "# CHARPROTSET = { 'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4, 'G': 5, 'H': 6, \\\n",
        "#             'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11, 'P': 12, 'Q': 13, \\\n",
        "#             'R': 14, 'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19, 'X': 20, \\\n",
        "#             'O': 20, 'U': 20,\n",
        "#             'B': (2, 11),\n",
        "#             'Z': (3, 13),\n",
        "#             'J': (7, 9) }\n",
        "# CHARPROTLEN = 21\n",
        "\n",
        "CHARPROTSET = { \"A\": 1, \"C\": 2, \"B\": 3, \"E\": 4, \"D\": 5, \"G\": 6,\n",
        "\t\t\t\t\"F\": 7, \"I\": 8, \"H\": 9, \"K\": 10, \"M\": 11, \"L\": 12,\n",
        "\t\t\t\t\"O\": 13, \"N\": 14, \"Q\": 15, \"P\": 16, \"S\": 17, \"R\": 18,\n",
        "\t\t\t\t\"U\": 19, \"T\": 20, \"W\": 21,\n",
        "\t\t\t\t\"V\": 22, \"Y\": 23, \"X\": 24,\n",
        "\t\t\t\t\"Z\": 25 }\n",
        "\n",
        "CHARPROTLEN = 25\n",
        "\n",
        "CHARCANSMISET = { \"#\": 1, \"%\": 2, \")\": 3, \"(\": 4, \"+\": 5, \"-\": 6,\n",
        "                 \".\": 7, \"1\": 8, \"0\": 9, \"3\": 10, \"2\": 11, \"5\": 12,\n",
        "                 \"4\": 13, \"7\": 14, \"6\": 15, \"9\": 16, \"8\": 17, \"=\": 18,\n",
        "                 \"A\": 19, \"C\": 20, \"B\": 21, \"E\": 22, \"D\": 23, \"G\": 24,\n",
        "                 \"F\": 25, \"I\": 26, \"H\": 27, \"K\": 28, \"M\": 29, \"L\": 30,\n",
        "                 \"O\": 31, \"N\": 32, \"P\": 33, \"S\": 34, \"R\": 35, \"U\": 36,\n",
        "                 \"T\": 37, \"W\": 38, \"V\": 39, \"Y\": 40, \"[\": 41, \"Z\": 42,\n",
        "                 \"]\": 43, \"_\": 44, \"a\": 45, \"c\": 46, \"b\": 47, \"e\": 48,\n",
        "                 \"d\": 49, \"g\": 50, \"f\": 51, \"i\": 52, \"h\": 53, \"m\": 54,\n",
        "                 \"l\": 55, \"o\": 56, \"n\": 57, \"s\": 58, \"r\": 59, \"u\": 60,\n",
        "                 \"t\": 61, \"y\": 62}\n",
        "\n",
        "CHARCANSMILEN = 62\n",
        "\n",
        "CHARISOSMISET = {\"#\": 29, \"%\": 30, \")\": 31, \"(\": 1, \"+\": 32, \"-\": 33, \"/\": 34, \".\": 2,\n",
        "\t\t\t\t\"1\": 35, \"0\": 3, \"3\": 36, \"2\": 4, \"5\": 37, \"4\": 5, \"7\": 38, \"6\": 6,\n",
        "\t\t\t\t\"9\": 39, \"8\": 7, \"=\": 40, \"A\": 41, \"@\": 8, \"C\": 42, \"B\": 9, \"E\": 43,\n",
        "\t\t\t\t\"D\": 10, \"G\": 44, \"F\": 11, \"I\": 45, \"H\": 12, \"K\": 46, \"M\": 47, \"L\": 13,\n",
        "\t\t\t\t\"O\": 48, \"N\": 14, \"P\": 15, \"S\": 49, \"R\": 16, \"U\": 50, \"T\": 17, \"W\": 51,\n",
        "\t\t\t\t\"V\": 18, \"Y\": 52, \"[\": 53, \"Z\": 19, \"]\": 54, \"\\\\\": 20, \"a\": 55, \"c\": 56,\n",
        "\t\t\t\t\"b\": 21, \"e\": 57, \"d\": 22, \"g\": 58, \"f\": 23, \"i\": 59, \"h\": 24, \"m\": 60,\n",
        "\t\t\t\t\"l\": 25, \"o\": 61, \"n\": 26, \"s\": 62, \"r\": 27, \"u\": 63, \"t\": 28, \"y\": 64}\n",
        "\n",
        "CHARISOSMILEN = 64\n",
        "\n",
        "\n",
        "## ######################## ##\n",
        "#\n",
        "#  Encoding Helpers\n",
        "#\n",
        "## ######################## ##\n",
        "\n",
        "#  Y = -(np.log10(Y/(math.pow(math.e,9))))\n",
        "\n",
        "def one_hot_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
        "\tX = np.zeros((MAX_SMI_LEN, len(smi_ch_ind))) #+1\n",
        "\n",
        "\tfor i, ch in enumerate(line[:MAX_SMI_LEN]):\n",
        "\t\tX[i, (smi_ch_ind[ch]-1)] = 1\n",
        "\n",
        "\treturn X #.tolist()\n",
        "\n",
        "def one_hot_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
        "\tX = np.zeros((MAX_SEQ_LEN, len(smi_ch_ind)))\n",
        "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
        "\t\tX[i, (smi_ch_ind[ch])-1] = 1\n",
        "\n",
        "\treturn X #.tolist()\n",
        "\n",
        "\n",
        "def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):\n",
        "\tX = np.zeros(MAX_SMI_LEN)\n",
        "\tfor i, ch in enumerate(line[:MAX_SMI_LEN]): #\tx, smi_ch_ind, y\n",
        "\t\tX[i] = smi_ch_ind[ch]\n",
        "\n",
        "\treturn X #.tolist()\n",
        "\n",
        "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
        "\tX = np.zeros(MAX_SEQ_LEN)\n",
        "\n",
        "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
        "\t\tX[i] = smi_ch_ind[ch]\n",
        "\n",
        "\treturn X #.tolist()\n",
        "\n",
        "\n",
        "\n",
        "## ######################## ##\n",
        "#\n",
        "#  DATASET Class\n",
        "#\n",
        "## ######################## ##\n",
        "# works for large dataset\n",
        "class DataSet(object):\n",
        "  def __init__(self, fpath, setting_no, seqlen, smilen, need_shuffle = False):\n",
        "    self.SEQLEN = seqlen\n",
        "    self.SMILEN = smilen\n",
        "    #self.NCLASSES = n_classes\n",
        "    self.charseqset = CHARPROTSET\n",
        "    self.charseqset_size = CHARPROTLEN\n",
        "\n",
        "    self.charsmiset = CHARISOSMISET ###HERE CAN BE EDITED\n",
        "    self.charsmiset_size = CHARISOSMILEN\n",
        "    self.PROBLEMSET = setting_no\n",
        "\n",
        "    # read raw file\n",
        "    # self._raw = self.read_sets( FLAGS)\n",
        "\n",
        "    # iteration flags\n",
        "    # self._num_data = len(self._raw)\n",
        "\n",
        "\n",
        "  def read_sets(self, FLAGS): ### fpath should be the dataset folder /kiba/ or /davis/\n",
        "    fpath = '/content/DeepDTA/DeepDTA/'\n",
        "    setting_no = FLAGS.problem_type\n",
        "    print(\"Reading %s start\" % fpath)\n",
        "\n",
        "    test_fold = json.load(open(\"/content/DeepDTA/data/kiba/folds/test_fold_setting1.txt\" +\n",
        "                               str(setting_no)+\".txt\"))\n",
        "    train_folds = json.load(open(fpath + \"folds/train_fold_setting\" +\n",
        "                                 str(setting_no)+\".txt\"))\n",
        "\n",
        "    return test_fold, train_folds\n",
        "\n",
        "  def parse_data(self, FLAGS,  with_label=True):\n",
        "    fpath = FLAGS.dataset_path\n",
        "    print(\"Read %s start\" % fpath)\n",
        "\n",
        "\n",
        "    ligands = json.load(open(\"/content/DeepDTA/data/kiba/ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
        "    proteins = json.load(open(\"/content/DeepDTA/data/kiba/proteins.txt\"), object_pairs_hook=OrderedDict)\n",
        "\n",
        "    #ligands = json.load(open(fpath+\"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
        "    #proteins = json.load(open(fpath+\"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
        "\n",
        "    Y = pickle.load(open(\"/content/DeepDTA/data/kiba/Y\",\"rb\"), encoding='latin1') ### TODO: read from raw\n",
        "    if FLAGS.is_log:\n",
        "        Y = -(np.log10(Y/(math.pow(10,9))))\n",
        "\n",
        "    XD = []\n",
        "    XT = []\n",
        "\n",
        "    if with_label:\n",
        "        for d in ligands.keys():\n",
        "            XD.append(label_smiles(ligands[d], self.SMILEN, self.charsmiset))\n",
        "\n",
        "        for t in proteins.keys():\n",
        "            XT.append(label_sequence(proteins[t], self.SEQLEN, self.charseqset))\n",
        "    else:\n",
        "        for d in ligands.keys():\n",
        "            XD.append(one_hot_smiles(ligands[d], self.SMILEN, self.charsmiset))\n",
        "\n",
        "        for t in proteins.keys():\n",
        "            XT.append(one_hot_sequence(proteins[t], self.SEQLEN, self.charseqset))\n",
        "\n",
        "    return XD, XT, Y"
      ],
      "metadata": {
        "id": "oprZUdgK6BZP"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product"
      ],
      "metadata": {
        "id": "3mmzvM_k67to"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from sys import argv\n",
        "import logging\n",
        "import argparse\n",
        "import plotly"
      ],
      "metadata": {
        "id": "WJVYbtQL7SwD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import ELU, PReLU, LeakyReLU\n",
        "from keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "\n",
        "from keras.layers import Conv2D, GRU\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers, layers"
      ],
      "metadata": {
        "id": "nDGovVSmYzZG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, pickle, os\n",
        "import math, json, time\n",
        "import decimal\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from random import shuffle\n",
        "from copy import deepcopy\n",
        "from sklearn import preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "8nf50bCi8av-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from emetrics import get_aupr, get_cindex, get_rm2\n",
        "import subprocess\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_aupr(Y, P):\n",
        "    if hasattr(Y, 'A'):\n",
        "        Y = Y.A\n",
        "    if hasattr(P, 'A'):\n",
        "        P = P.A\n",
        "    Y = np.where(Y>0, 1, 0)\n",
        "    Y = Y.ravel()\n",
        "    P = P.ravel()\n",
        "    f = open(\"temp.txt\", 'w')\n",
        "    for i in range(Y.shape[0]):\n",
        "        f.write(\"%f %d\\n\" %(P[i], Y[i]))\n",
        "    f.close()\n",
        "    f = open(\"foo.txt\", 'w')\n",
        "    subprocess.call([\"java\", \"-jar\", \"auc.jar\", \"temp.txt\", \"list\"], stdout=f)\n",
        "    f.close()\n",
        "    f = open(\"foo.txt\")\n",
        "    lines = f.readlines()\n",
        "    aucpr = float(lines[-2].split()[-1])\n",
        "    f.close()\n",
        "    return aucpr\n",
        "\n",
        "\n",
        "\n",
        "def get_cindex(Y, P):\n",
        "    summ = 0\n",
        "    pair = 0\n",
        "\n",
        "    for i in range(1, len(Y)):\n",
        "        for j in range(0, i):\n",
        "            if i is not j:\n",
        "                if(Y[i] > Y[j]):\n",
        "                    pair +=1\n",
        "                    summ +=  1* (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n",
        "\n",
        "\n",
        "    if pair != 0:\n",
        "        return summ/pair\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def r_squared_error(y_obs,y_pred):\n",
        "    y_obs = np.array(y_obs)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
        "    y_pred_mean = [np.mean(y_pred) for y in y_pred]\n",
        "\n",
        "    mult = sum((y_pred - y_pred_mean) * (y_obs - y_obs_mean))\n",
        "    mult = mult * mult\n",
        "\n",
        "    y_obs_sq = sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
        "    y_pred_sq = sum((y_pred - y_pred_mean) * (y_pred - y_pred_mean) )\n",
        "\n",
        "    return mult / float(y_obs_sq * y_pred_sq)\n",
        "\n",
        "\n",
        "def get_k(y_obs,y_pred):\n",
        "    y_obs = np.array(y_obs)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    return sum(y_obs*y_pred) / float(sum(y_pred*y_pred))\n",
        "\n",
        "\n",
        "def squared_error_zero(y_obs,y_pred):\n",
        "    k = get_k(y_obs,y_pred)\n",
        "\n",
        "    y_obs = np.array(y_obs)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_obs_mean = [np.mean(y_obs) for y in y_obs]\n",
        "    upp = sum((y_obs - (k*y_pred)) * (y_obs - (k* y_pred)))\n",
        "    down= sum((y_obs - y_obs_mean)*(y_obs - y_obs_mean))\n",
        "\n",
        "    return 1 - (upp / float(down))\n",
        "\n",
        "\n",
        "def get_rm2(ys_orig,ys_line):\n",
        "    r2 = r_squared_error(ys_orig, ys_line)\n",
        "    r02 = squared_error_zero(ys_orig, ys_line)\n",
        "\n",
        "    return r2 * (1 - np.sqrt(np.absolute((r2*r2)-(r02*r02))))"
      ],
      "metadata": {
        "id": "wEQtZTHd5tSY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import from srguments\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def argparser():\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # for model\n",
        "  parser.add_argument(\n",
        "      '--seq_window_lengths',\n",
        "      type=int,\n",
        "      nargs='+',\n",
        "      help='Space seperated list of motif filter lengths. (ex, --window_lengths 4 8 12)'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--smi_window_lengths',\n",
        "      type=int,\n",
        "      nargs='+',\n",
        "      help='Space seperated list of motif filter lengths. (ex, --window_lengths 4 8 12)'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--num_windows',\n",
        "      type=int,\n",
        "      nargs='+',\n",
        "      help='Space seperated list of the number of motif filters corresponding to length list. (ex, --num_windows 100 200 100)'  # noqa: E501\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--num_hidden',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help='Number of neurons in hidden layer.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--num_classes',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help='Number of classes (families).'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--max_seq_len',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help='Length of input sequences.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--max_smi_len',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help='Length of input sequences.'\n",
        "  )\n",
        "  # for learning\n",
        "  parser.add_argument(\n",
        "      '--learning_rate',\n",
        "      type=float,\n",
        "      default=0.001,\n",
        "      help='Initial learning rate.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--num_epoch',\n",
        "      type=int,\n",
        "      default=100,\n",
        "      help='Number of epochs to train.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--batch_size',\n",
        "      type=int,\n",
        "      default=256,\n",
        "      help='Batch size. Must divide evenly into the dataset sizes.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--dataset_path',\n",
        "      type=str,\n",
        "      default='/data/kiba/',\n",
        "      help='Directory for input data.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--problem_type',\n",
        "      type=int,\n",
        "      default=1,\n",
        "      help='Type of the prediction problem (1-4)'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--binary_th',\n",
        "      type=float,\n",
        "      default=0.0,\n",
        "      help='Threshold to split data into binary classes'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--is_log',\n",
        "      type=int,\n",
        "      default=0,\n",
        "      help='use log transformation for Y'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--checkpoint_path',\n",
        "      type=str,\n",
        "      default='',\n",
        "      help='Path to write checkpoint file.'\n",
        "  )\n",
        "  parser.add_argument(\n",
        "      '--log_dir',\n",
        "      type=str,\n",
        "      default='/tmp',\n",
        "      help='Directory for log data.'\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  FLAGS, unparsed = parser.parse_known_args()\n",
        "\n",
        "  # check validity\n",
        "  #assert( len(FLAGS.window_lengths) == len(FLAGS.num_windows) )\n",
        "\n",
        "  return FLAGS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def logging(msg, FLAGS):\n",
        "  fpath = os.path.join( FLAGS.log_dir, \"log.txt\" )\n",
        "  with open( fpath, \"a\" ) as fw:\n",
        "    fw.write(\"%s\\n\" % msg)\n",
        "  #print(msg)"
      ],
      "metadata": {
        "id": "sVT_a63r5uz_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TABSY = \"\\t\"\n",
        "figdir = \"figures/\""
      ],
      "metadata": {
        "id": "s5ZqF3VKcZYB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_combined_categorical(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "\n",
        "    XDinput = Input(shape=(FLAGS.max_smi_len,), dtype='int32') ### Buralar flagdan gelmeliii\n",
        "    XTinput = Input(shape=(FLAGS.max_seq_len,), dtype='int32')\n",
        "\n",
        "    ### SMI_EMB_DINMS  FLAGS GELMELII\n",
        "    encode_smiles = Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len)(XDinput)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = GlobalMaxPooling1D()(encode_smiles)\n",
        "\n",
        "\n",
        "    encode_protein = Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128, input_length=FLAGS.max_seq_len)(XTinput)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = GlobalMaxPooling1D()(encode_protein)\n",
        "\n",
        "\n",
        "    encode_interaction = keras.layers.concatenate([encode_smiles, encode_protein], axis=-1) #merge.Add()([encode_smiles, encode_protein])\n",
        "\n",
        "    # Fully connected\n",
        "    FC1 = Dense(1024, activation='relu')(encode_interaction)\n",
        "    FC2 = Dropout(0.1)(FC1)\n",
        "    FC2 = Dense(1024, activation='relu')(FC2)\n",
        "    FC2 = Dropout(0.1)(FC2)\n",
        "    FC2 = Dense(512, activation='relu')(FC2)\n",
        "\n",
        "\n",
        "    # And add a logistic regression on top\n",
        "    predictions = Dense(1, kernel_initializer='normal')(FC2) #OR no activation, rght now it's between 0-1, do I want this??? activation='sigmoid'\n",
        "\n",
        "    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])\n",
        "\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score]) #, metrics=['cindex_score']\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_combined_categorical.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "-GSkrQK4e0y0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_single_drug(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "\n",
        "    interactionModel = Sequential()\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Activation('linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    encode_smiles = Sequential()\n",
        "    encode_smiles.add(Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len))\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)) #input_shape=(MAX_SMI_LEN, SMI_EMBEDDING_DIMS)\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\n",
        "    encode_smiles.add(GlobalMaxPooling1D())\n",
        "\n",
        "    # Merge doesnt work in tensorflow 2\n",
        "    #interactionModel.add(Merge([encode_smiles, XTmodel], mode='concat', concat_axis=1))\n",
        "    #interactionModel.add(layers.merge.Concatenate([XDmodel, XTmodel]))\n",
        "    interactionModel.add(layers.Concatenate([encode_smiles, XTmodel]))\n",
        "\n",
        "\n",
        "    # example tf.keras.layers.Concatenate(axis=1)([encode_smiles, XTmodel])\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu')) #1024\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu')) #1024\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_single_drug.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "5GuzGviMe3-b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_single_prot(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "\n",
        "    interactionModel = Sequential()\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Activation('linear', input_shape=(FLAGS.drugcount,)))\n",
        "\n",
        "\n",
        "    XTmodel1 = Sequential()\n",
        "    XTmodel1.add(Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128,  input_length=FLAGS.max_seq_len))\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)) #input_shape=(MAX_SEQ_LEN, SEQ_EMBEDDING_DIMS)\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1))\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1))\n",
        "    XTmodel1.add(GlobalMaxPooling1D())\n",
        "\n",
        "    interactionModel.add(layers.Concatenate([XDmodel, XTmodel1]))\n",
        "\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_single_protein.png')\n",
        "\n",
        "    return interactionModel\n"
      ],
      "metadata": {
        "id": "SRLLipRkfCVP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "    interactionModel = Sequential()\n",
        "\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.drug_count, )))\n",
        "\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    interactionModel.add(layers.Concatenate([XDmodel, XTmodel]))\n",
        "\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_baseline.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "UVRKjUvQfJvb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "    interactionModel = Sequential()\n",
        "\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.drug_count, )))\n",
        "\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    interactionModel.add(layers.Concatenate([XDmodel, XTmodel]))\n",
        "\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_baseline.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "Q9PiUzBEfW8f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nfold_1_2_3_setting_sample(XD, XT,  Y, label_row_inds, label_col_inds, measure, runmethod,  FLAGS, dataset):\n",
        "\n",
        "    bestparamlist = []\n",
        "    test_set, outer_train_sets = dataset.read_sets(FLAGS)\n",
        "\n",
        "    foldinds = len(outer_train_sets)\n",
        "\n",
        "    test_sets = []\n",
        "    ## TRAIN AND VAL\n",
        "    val_sets = []\n",
        "    train_sets = []\n",
        "\n",
        "    #logger.info('Start training')\n",
        "    for val_foldind in range(foldinds):\n",
        "        val_fold = outer_train_sets[val_foldind]\n",
        "        val_sets.append(val_fold)\n",
        "        otherfolds = deepcopy(outer_train_sets)\n",
        "        otherfolds.pop(val_foldind)\n",
        "        otherfoldsinds = [item for sublist in otherfolds for item in sublist]\n",
        "        train_sets.append(otherfoldsinds)\n",
        "        test_sets.append(test_set)\n",
        "        print(\"val set\", str(len(val_fold)))\n",
        "        print(\"train set\", str(len(otherfoldsinds)))\n",
        "    bestparamind, best_param_list, bestperf, all_predictions_not_need, losses_not_need = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds,\n",
        "                                                                                                measure, runmethod, FLAGS, train_sets, val_sets)\n",
        "    #print(\"Test Set len\", str(len(test_set)))\n",
        "    #print(\"Outer Train Set len\", str(len(outer_train_sets)))\n",
        "    bestparam, best_param_list, bestperf, all_predictions, all_losses = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds,\n",
        "                                                                                                measure, runmethod, FLAGS, train_sets, test_sets)\n",
        "\n",
        "    testperf = all_predictions[bestparamind]##pointer pos\n",
        "\n",
        "    logging(\"---FINAL RESULTS-----\", FLAGS)\n",
        "    logging(\"best param index = %s,  best param = %.5f\" %\n",
        "            (bestparamind, bestparam), FLAGS)\n",
        "\n",
        "\n",
        "    testperfs = []\n",
        "    testloss= []\n",
        "\n",
        "    avgperf = 0.\n",
        "\n",
        "    for test_foldind in range(len(test_sets)):\n",
        "        foldperf = all_predictions[bestparamind][test_foldind]\n",
        "        foldloss = all_losses[bestparamind][test_foldind]\n",
        "        testperfs.append(foldperf)\n",
        "        testloss.append(foldloss)\n",
        "        avgperf += foldperf\n",
        "\n",
        "    avgperf = avgperf / len(test_sets)\n",
        "    avgloss = np.mean(testloss)\n",
        "    teststd = np.std(testperfs)\n",
        "\n",
        "    logging(\"Test Performance CI\", FLAGS)\n",
        "    logging(testperfs, FLAGS)\n",
        "    logging(\"Test Performance MSE\", FLAGS)\n",
        "    logging(testloss, FLAGS)\n",
        "\n",
        "    return avgperf, avgloss, teststd"
      ],
      "metadata": {
        "id": "eOz_h_yLfqiR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified #general_nfold_cv\n",
        "def general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets): ## BURAYA DA FLAGS LAZIM????\n",
        "\n",
        "    paramset1 = FLAGS.num_windows                              #[32]#[32,  512] #[32, 128]  # filter numbers\n",
        "    paramset2 = FLAGS.smi_window_lengths                               #[4, 8]#[4,  32] #[4,  8] #filter length smi\n",
        "    paramset3 = FLAGS.seq_window_lengths                               #[8, 12]#[64,  256] #[64, 192]#[8, 192, 384]\n",
        "    epoch = FLAGS.num_epoch                                 #100\n",
        "    batchsz = FLAGS.batch_size                             #256\n",
        "\n",
        "    logging(\"---Parameter Search-----\", FLAGS)\n",
        "\n",
        "    # w = len(val_sets)\n",
        "    # h = len(paramset1) * len(paramset2) * len(paramset3)\n",
        "\n",
        "    # all_predictions = [[0 for x in range(w)] for y in range(h)]\n",
        "    # all_losses = [[0 for x in range(w)] for y in range(h)]\n",
        "    # print(all_predictions)\n",
        "\n",
        "    for foldind in range(len(val_sets)):\n",
        "        valinds = val_sets[foldind]\n",
        "        labeledinds = labeled_sets[foldind]\n",
        "\n",
        "        Y_train = np.mat(np.copy(Y))\n",
        "\n",
        "        params = {}\n",
        "        XD_train = XD\n",
        "        XT_train = XT\n",
        "        trrows = label_row_inds[labeledinds]\n",
        "        trcols = label_col_inds[labeledinds]\n",
        "\n",
        "        XD_train = XD[trrows]\n",
        "        XT_train = XT[trcols]\n",
        "\n",
        "        train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, trrows, trcols)\n",
        "\n",
        "        terows = label_row_inds[valinds]\n",
        "        tecols = label_col_inds[valinds]\n",
        "        #print(\"terows\", str(terows), str(len(terows)))\n",
        "        #print(\"tecols\", str(tecols), str(len(tecols)))\n",
        "\n",
        "        val_drugs, val_prots,  val_Y = prepare_interaction_pairs(XD, XT,  Y, terows, tecols)\n",
        "\n",
        "\n",
        "        pointer = 0\n",
        "\n",
        "        #for param1ind in range(len(paramset1)): #hidden neurons\n",
        "        param1value = paramset1[0]\n",
        "            #for param2ind in range(len(paramset2)): #learning rate\n",
        "        param2value = paramset2[0]\n",
        "\n",
        "                #for param3ind in range(len(paramset3)):\n",
        "        param3value = paramset3[0]\n",
        "\n",
        "        gridmodel = runmethod(FLAGS, param1value, param2value, param3value)\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "        gridres = gridmodel.fit(([np.array(train_drugs),np.array(train_prots) ]), np.array(train_Y), batch_size=batchsz, epochs=epoch,\n",
        "                validation_data=( ([np.array(val_drugs), np.array(val_prots) ]), np.array(val_Y)),  shuffle=False, callbacks=[es] )\n",
        "\n",
        "\n",
        "        predicted_labels = gridmodel.predict([np.array(val_drugs), np.array(val_prots) ])\n",
        "        loss, rperf2 = gridmodel.evaluate(([np.array(val_drugs),np.array(val_prots) ]), np.array(val_Y), verbose=0)\n",
        "        rperf = prfmeasure(val_Y, predicted_labels)\n",
        "        rperf = rperf[0]\n",
        "\n",
        "\n",
        "        logging(\"P1 = %d,  P2 = %d, P3 = %d, Fold = %d, CI-i = %f, CI-ii = %f, MSE = %f\" %\n",
        "        (param1ind, param2ind, param3ind, foldind, rperf, rperf2, loss), FLAGS)\n",
        "\n",
        "        plotLoss(gridres, param1ind, param2ind, param3ind, foldind)\n",
        "\n",
        "        all_predictions[pointer][foldind] =rperf #TODO FOR EACH VAL SET allpredictions[pointer][foldind]\n",
        "        all_losses[pointer][foldind]= loss\n",
        "\n",
        "        pointer +=1\n",
        "\n",
        "    bestperf = -float('Inf')\n",
        "    bestpointer = None\n",
        "\n",
        "\n",
        "    best_param_list = []\n",
        "    ##Take average according to folds, then chooose best params\n",
        "    pointer = 0\n",
        "    for param1ind in range(len(paramset1)):\n",
        "            for param2ind in range(len(paramset2)):\n",
        "                for param3ind in range(len(paramset3)):\n",
        "\n",
        "                    avgperf = 0.\n",
        "                    for foldind in range(len(val_sets)):\n",
        "                        foldperf = all_predictions[pointer][foldind]\n",
        "                        avgperf += foldperf\n",
        "                    avgperf /= len(val_sets)\n",
        "                    #print(epoch, batchsz, avgperf)\n",
        "                    if avgperf > bestperf:\n",
        "                        bestperf = avgperf\n",
        "                        bestpointer = pointer\n",
        "                        best_param_list = [param1ind, param2ind, param3ind]\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    return  bestpointer, best_param_list, bestperf, all_predictions, all_losses"
      ],
      "metadata": {
        "id": "P91sATmLfuSu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets): ## BURAYA DA FLAGS LAZIM????\n",
        "\n",
        "    paramset1 = FLAGS.num_windows                              #[32]#[32,  512] #[32, 128]  # filter numbers\n",
        "    paramset2 = FLAGS.smi_window_lengths                               #[4, 8]#[4,  32] #[4,  8] #filter length smi\n",
        "    paramset3 = FLAGS.seq_window_lengths                               #[8, 12]#[64,  256] #[64, 192]#[8, 192, 384]\n",
        "    epoch = FLAGS.num_epoch                                 #100\n",
        "    batchsz = FLAGS.batch_size                             #256\n",
        "\n",
        "    logging(\"---Parameter Search-----\", FLAGS)\n",
        "\n",
        "    w = len(val_sets)\n",
        "    h = len(paramset1) * len(paramset2) * len(paramset3)\n",
        "\n",
        "    all_predictions = [[0 for x in range(w)] for y in range(h)]\n",
        "    all_losses = [[0 for x in range(w)] for y in range(h)]\n",
        "    print(all_predictions)\n",
        "\n",
        "    for foldind in range(len(val_sets)):\n",
        "        valinds = val_sets[foldind]\n",
        "        labeledinds = labeled_sets[foldind]\n",
        "\n",
        "        Y_train = np.mat(np.copy(Y))\n",
        "\n",
        "        params = {}\n",
        "        XD_train = XD\n",
        "        XT_train = XT\n",
        "        trrows = label_row_inds[labeledinds]\n",
        "        trcols = label_col_inds[labeledinds]\n",
        "\n",
        "        XD_train = XD[trrows]\n",
        "        XT_train = XT[trcols]\n",
        "\n",
        "        train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, trrows, trcols)\n",
        "\n",
        "        terows = label_row_inds[valinds]\n",
        "        tecols = label_col_inds[valinds]\n",
        "        #print(\"terows\", str(terows), str(len(terows)))\n",
        "        #print(\"tecols\", str(tecols), str(len(tecols)))\n",
        "\n",
        "        val_drugs, val_prots,  val_Y = prepare_interaction_pairs(XD, XT,  Y, terows, tecols)\n",
        "\n",
        "\n",
        "        pointer = 0\n",
        "\n",
        "        for param1ind in range(len(paramset1)): #hidden neurons\n",
        "            param1value = paramset1[param1ind]\n",
        "            for param2ind in range(len(paramset2)): #learning rate\n",
        "                param2value = paramset2[param2ind]\n",
        "\n",
        "                for param3ind in range(len(paramset3)):\n",
        "                    param3value = paramset3[param3ind]\n",
        "\n",
        "                    gridmodel = runmethod(FLAGS, param1value, param2value, param3value)\n",
        "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "                    gridres = gridmodel.fit(([np.array(train_drugs),np.array(train_prots) ]), np.array(train_Y), batch_size=batchsz, epochs=epoch,\n",
        "                            validation_data=( ([np.array(val_drugs), np.array(val_prots) ]), np.array(val_Y)),  shuffle=False, callbacks=[es] )\n",
        "\n",
        "\n",
        "                    predicted_labels = gridmodel.predict([np.array(val_drugs), np.array(val_prots) ])\n",
        "                    loss, rperf2 = gridmodel.evaluate(([np.array(val_drugs),np.array(val_prots) ]), np.array(val_Y), verbose=0)\n",
        "                    rperf = prfmeasure(val_Y, predicted_labels)\n",
        "                    rperf = rperf[0]\n",
        "\n",
        "\n",
        "                    logging(\"P1 = %d,  P2 = %d, P3 = %d, Fold = %d, CI-i = %f, CI-ii = %f, MSE = %f\" %\n",
        "                    (param1ind, param2ind, param3ind, foldind, rperf, rperf2, loss), FLAGS)\n",
        "\n",
        "                    plotLoss(gridres, param1ind, param2ind, param3ind, foldind)\n",
        "\n",
        "                    all_predictions[pointer][foldind] =rperf #TODO FOR EACH VAL SET allpredictions[pointer][foldind]\n",
        "                    all_losses[pointer][foldind]= loss\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    bestperf = -float('Inf')\n",
        "    bestpointer = None\n",
        "\n",
        "\n",
        "    best_param_list = []\n",
        "    ##Take average according to folds, then chooose best params\n",
        "    pointer = 0\n",
        "    for param1ind in range(len(paramset1)):\n",
        "            for param2ind in range(len(paramset2)):\n",
        "                for param3ind in range(len(paramset3)):\n",
        "\n",
        "                    avgperf = 0.\n",
        "                    for foldind in range(len(val_sets)):\n",
        "                        foldperf = all_predictions[pointer][foldind]\n",
        "                        avgperf += foldperf\n",
        "                    avgperf /= len(val_sets)\n",
        "                    #print(epoch, batchsz, avgperf)\n",
        "                    if avgperf > bestperf:\n",
        "                        bestperf = avgperf\n",
        "                        bestpointer = pointer\n",
        "                        best_param_list = [param1ind, param2ind, param3ind]\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    return  bestpointer, best_param_list, bestperf, all_predictions, all_losses\n"
      ],
      "metadata": {
        "id": "PIlr0kArALJ9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cindex_score(y_true, y_pred):\n",
        "\n",
        "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
        "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
        "\n",
        "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
        "    f = tf.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
        "\n",
        "    g = tf.reduce_sum(tf.multiply(g, f))\n",
        "    f = tf.reduce_sum(f)\n",
        "\n",
        "    return tf.where(tf.equal(g, 0), 0.0, g/f) #select\n",
        "\n",
        "def plotLoss(history, batchind, epochind, param3ind, foldind):\n",
        "\n",
        "    figname = \"b\"+str(batchind) + \"_e\" + str(epochind) + \"_\" + str(param3ind) + \"_\"  + str( foldind) + \"_\" + str(time.time())\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\t#plt.legend(['trainloss', 'valloss', 'cindex', 'valcindex'], loc='upper left')\n",
        "    plt.legend(['trainloss', 'valloss'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname +\".png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait',\n",
        "                    papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "    ## PLOT CINDEX\n",
        "    plt.figure()\n",
        "    plt.title('model concordance index')\n",
        "    plt.ylabel('cindex')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.plot(history.history['cindex_score'])\n",
        "    plt.plot(history.history['val_cindex_score'])\n",
        "    plt.legend(['traincindex', 'valcindex'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname + \"_acc.png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait',\n",
        "                            papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "def prepare_interaction_pairs(XD, XT,  Y, rows, cols):\n",
        "    drugs = []\n",
        "    targets = []\n",
        "    targetscls = []\n",
        "    affinity=[]\n",
        "\n",
        "    for pair_ind in range(len(rows)):\n",
        "        drug = XD[rows[pair_ind]]\n",
        "        drugs.append(drug)\n",
        "\n",
        "        target=XT[cols[pair_ind]]\n",
        "        targets.append(target)\n",
        "\n",
        "        affinity.append(Y[rows[pair_ind],cols[pair_ind]])\n",
        "\n",
        "    drug_data = np.stack(drugs)\n",
        "    target_data = np.stack(targets)\n",
        "\n",
        "    return drug_data,target_data,affinity"
      ],
      "metadata": {
        "id": "Che6u4z1f3cq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(FLAGS, perfmeasure, deepmethod, foldcount=6): #5-fold cross validation + test\n",
        "\n",
        "    #Input\n",
        "    #XD: [drugs, features] sized array (features may also be similarities with other drugs\n",
        "    #XT: [targets, features] sized array (features may also be similarities with other targets\n",
        "    #Y: interaction values, can be real values or binary (+1, -1), insert value float(\"nan\") for unknown entries\n",
        "    #perfmeasure: function that takes as input a list of correct and predicted outputs, and returns performance\n",
        "    #higher values should be better, so if using error measures use instead e.g. the inverse -error(Y, P)\n",
        "    #foldcount: number of cross-validation folds for settings 1-3, setting 4 always runs 3x3 cross-validation\n",
        "\n",
        "\n",
        "    dataset = DataSet( fpath = FLAGS.dataset_path, ### BUNU ARGS DA GUNCELLE\n",
        "                      setting_no = FLAGS.problem_type, ##BUNU ARGS A EKLE\n",
        "                      seqlen = FLAGS.max_seq_len,\n",
        "                      smilen = FLAGS.max_smi_len,\n",
        "                      need_shuffle = False)\n",
        "\n",
        "\n",
        "    # set character set size\n",
        "    FLAGS.charseqset_size = dataset.charseqset_size\n",
        "    FLAGS.charsmiset_size = dataset.charsmiset_size\n",
        "\n",
        "    XD, XT, Y = dataset.parse_data(FLAGS)\n",
        "\n",
        "    XD = np.asarray(XD)\n",
        "    XT = np.asarray(XT)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    drugcount = XD.shape[0]\n",
        "    print(drugcount)\n",
        "    targetcount = XT.shape[0]\n",
        "    print(targetcount)\n",
        "\n",
        "    FLAGS.drug_count = drugcount\n",
        "    FLAGS.target_count = targetcount\n",
        "\n",
        "    label_row_inds, label_col_inds = np.where(np.isnan(Y)==False)  #basically finds the point address of affinity [x,y]\n",
        "\n",
        "    if not os.path.exists(figdir):\n",
        "        os.makedirs(figdir)\n",
        "\n",
        "    print(FLAGS.log_dir)\n",
        "    S1_avgperf, S1_avgloss, S1_teststd = nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n",
        "                                                                     perfmeasure, deepmethod, FLAGS, dataset)\n",
        "\n",
        "    logging(\"Setting \" + str(FLAGS.problem_type), FLAGS)\n",
        "    logging(\"avg_perf = %.5f,  avg_mse = %.5f, std = %.5f\" %\n",
        "            (S1_avgperf, S1_avgloss, S1_teststd), FLAGS)"
      ],
      "metadata": {
        "id": "BI8FENWXf__0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_regression(FLAGS):\n",
        "\n",
        "    perfmeasure = get_cindex\n",
        "    deepmethod = build_combined_categorical\n",
        "\n",
        "    experiment(FLAGS, perfmeasure, deepmethod)"
      ],
      "metadata": {
        "id": "l_eOEUTdgDsJ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ArgumentParser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLH_UOQx-mrC",
        "outputId": "49cbba70-3838-4c41-e155-1f9e2e25bf73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ArgumentParser\n",
            "  Downloading argumentparser-1.2.1.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ArgumentParser\n",
            "  Building wheel for ArgumentParser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ArgumentParser: filename=argumentparser-1.2.1-py3-none-any.whl size=4849 sha256=cb9449dbfe9eac3d556beef740963247c76b7d21e829e71d9f7a0180b2136283\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/6f/58/0fa85fa0943152d1ea243cd4fea2ea6c25136f8972a8696df2\n",
            "Successfully built ArgumentParser\n",
            "Installing collected packages: ArgumentParser\n",
            "Successfully installed ArgumentParser-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "import configparser"
      ],
      "metadata": {
        "id": "-pRy_BO19W6u"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    FLAGS = argparse.ArgumentParser()\n",
        "    FLAGS.num_windows = [32]\n",
        "    FLAGS.seq_window_lengths = [8,12]\n",
        "    FLAGS.smi_window_lengths = [4,8]\n",
        "    FLAGS.batch_size = [256]\n",
        "    FLAGS.num_epoch = [100]\n",
        "    FLAGS.max_seq_len = [1000]\n",
        "    FLAGS.max_smi_len = [100]\n",
        "    FLAGS.dataset_path = 'data/kiba/'\n",
        "    FLAGS.problem_type = [1]\n",
        "    FLAGS.log_dir = str(time.time()) + \"/\"\n",
        "\n",
        "    if not os.path.exists(FLAGS.log_dir):\n",
        "        os.makedirs(FLAGS.log_dir)\n",
        "\n",
        "    #logging(str(FLAGS), FLAGS)\n",
        "    #run_regression( FLAGS )"
      ],
      "metadata": {
        "id": "d6Dauy2KcbeP"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    FLAGS = argparser()\n",
        "    #FLAGS.log_dir = FLAGS.log_dir + str(time.time()) + \"/\"\n",
        "\n",
        "    # if not os.path.exists(FLAGS.log_dir):\n",
        "    #     os.makedirs(FLAGS.log_dir)\n",
        "\n",
        "    logging(str(FLAGS), FLAGS)\n",
        "    run_regression(FLAGS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F-su18N06jOS",
        "outputId": "d4f9ca85-ce71-430c-889c-a502917e9999"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read /data/kiba/ start\n",
            "2111\n",
            "229\n",
            "/tmp\n",
            "Reading /content/DeepDTA/DeepDTA/ start\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n",
            "val set 19709\n",
            "train set 78836\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-853e249a9618>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrun_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-74-a5822a600fea>\u001b[0m in \u001b[0;36mrun_regression\u001b[0;34m(FLAGS)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdeepmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_combined_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperfmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-93dd0fbd0ef5>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(FLAGS, perfmeasure, deepmethod, foldcount)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     S1_avgperf, S1_avgloss, S1_teststd = nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n\u001b[0m\u001b[1;32m     44\u001b[0m                                                                      perfmeasure, deepmethod, FLAGS, dataset)\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-1be70348f783>\u001b[0m in \u001b[0;36mnfold_1_2_3_setting_sample\u001b[0;34m(XD, XT, Y, label_row_inds, label_col_inds, measure, runmethod, FLAGS, dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train set\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0motherfoldsinds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     bestparamind, best_param_list, bestperf, all_predictions_not_need, losses_not_need = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds,\n\u001b[0m\u001b[1;32m     25\u001b[0m                                                                                                 measure, runmethod, FLAGS, train_sets, val_sets)\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#print(\"Test Set len\", str(len(test_set)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-d0491b72aea0>\u001b[0m in \u001b[0;36mgeneral_nfold_cv\u001b[0;34m(XD, XT, Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_sets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamset1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamset2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamset3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wa-Y-uD-CQ-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}