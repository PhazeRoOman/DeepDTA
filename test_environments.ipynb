{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWQugQn3rmw9Am69xbk1Vv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhazeRoOman/DeepDTA/blob/Menna's-branch/test_environments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9hycfGT26LV",
        "outputId": "15a48f00-a385-409b-c8ae-5f42bcdcc4b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wuyf_bHUltb_",
        "outputId": "071b4a59-2b12-45f0-c739-196745864cf2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtHmbkwyWs3u",
        "outputId": "a2254bf0-ea26-4386-da19-6921b775d67e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [834 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [849 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,103 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,235 kB]\n",
            "Fetched 4,412 kB in 5s (980 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Menna44/DeepDTA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxTmjKyV2p3V",
        "outputId": "3005680a-f903-4590-c251-da7c46ee7cf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepDTA'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 442 (delta 35), reused 29 (delta 29), pack-reused 398\u001b[K\n",
            "Receiving objects: 100% (442/442), 10.99 MiB | 6.08 MiB/s, done.\n",
            "Resolving deltas: 100% (176/176), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DeepDTA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6krld5726T9",
        "outputId": "9594b597-431d-4079-f569-a0600635bc2b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepDTA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TzrfO9vkaDx",
        "outputId": "45fcce14-2693-49db-c9b3-f7855873d0ef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepDTA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "#import matplotlib\n",
        "#matplotlib.use('Agg')\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "### We modified Pahikkala et al. (2014) source code for cross-val process ###\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "np.random.seed(1)\n",
        "rn.seed(1)"
      ],
      "metadata": {
        "id": "YOdeUf5W5kw1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n"
      ],
      "metadata": {
        "id": "1Fyv7a1W5pmR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datahelper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxjdJxrr60eP",
        "outputId": "f29bc6f8-f308-472a-e848-0ae1ca6fee64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'datahelper,'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datahelper import *\n",
        "from itertools import product"
      ],
      "metadata": {
        "id": "3mmzvM_k67to"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from sys import argv\n",
        "import logging\n",
        "import argparse\n",
        "import plotly"
      ],
      "metadata": {
        "id": "WJVYbtQL7SwD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import ELU, PReLU, LeakyReLU\n",
        "from keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "\n",
        "from keras.layers import Conv2D, GRU\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers, layers"
      ],
      "metadata": {
        "id": "nDGovVSmYzZG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, pickle, os\n",
        "import math, json, time\n",
        "import decimal\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from random import shuffle\n",
        "from copy import deepcopy\n",
        "from sklearn import preprocessing\n",
        "\n",
        "#from emetrics import get_aupr, get_cindex, get_rm2"
      ],
      "metadata": {
        "id": "8nf50bCi8av-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TABSY = \"\\t\"\n",
        "figdir = \"figures/\""
      ],
      "metadata": {
        "id": "s5ZqF3VKcZYB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_combined_categorical(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "\n",
        "    XDinput = Input(shape=(FLAGS.max_smi_len,), dtype='int32') ### Buralar flagdan gelmeliii\n",
        "    XTinput = Input(shape=(FLAGS.max_seq_len,), dtype='int32')\n",
        "\n",
        "    ### SMI_EMB_DINMS  FLAGS GELMELII\n",
        "    encode_smiles = Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len)(XDinput)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = GlobalMaxPooling1D()(encode_smiles)\n",
        "\n",
        "\n",
        "    encode_protein = Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128, input_length=FLAGS.max_seq_len)(XTinput)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = GlobalMaxPooling1D()(encode_protein)\n",
        "\n",
        "\n",
        "    encode_interaction = keras.layers.concatenate([encode_smiles, encode_protein], axis=-1) #merge.Add()([encode_smiles, encode_protein])\n",
        "\n",
        "    # Fully connected\n",
        "    FC1 = Dense(1024, activation='relu')(encode_interaction)\n",
        "    FC2 = Dropout(0.1)(FC1)\n",
        "    FC2 = Dense(1024, activation='relu')(FC2)\n",
        "    FC2 = Dropout(0.1)(FC2)\n",
        "    FC2 = Dense(512, activation='relu')(FC2)\n",
        "\n",
        "\n",
        "    # And add a logistic regression on top\n",
        "    predictions = Dense(1, kernel_initializer='normal')(FC2) #OR no activation, rght now it's between 0-1, do I want this??? activation='sigmoid'\n",
        "\n",
        "    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])\n",
        "\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score]) #, metrics=['cindex_score']\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_combined_categorical.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "-GSkrQK4e0y0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_single_drug(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "\n",
        "    interactionModel = Sequential()\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Activation('linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    encode_smiles = Sequential()\n",
        "    encode_smiles.add(Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len))\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)) #input_shape=(MAX_SMI_LEN, SMI_EMBEDDING_DIMS)\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\n",
        "    encode_smiles.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1))\n",
        "    encode_smiles.add(GlobalMaxPooling1D())\n",
        "\n",
        "    # Merge doesnt work in tensorflow 2\n",
        "    #interactionModel.add(Merge([encode_smiles, XTmodel], mode='concat', concat_axis=1))\n",
        "    #interactionModel.add(layers.merge.Concatenate([XDmodel, XTmodel]))\n",
        "    interactionModel.add(layers.Concatenate([encode_smiles, XTmodel]))\n",
        "\n",
        "\n",
        "    # example tf.keras.layers.Concatenate(axis=1)([encode_smiles, XTmodel])\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu')) #1024\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu')) #1024\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_single_drug.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "5GuzGviMe3-b"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_single_prot(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "\n",
        "    interactionModel = Sequential()\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Activation('linear', input_shape=(FLAGS.drugcount,)))\n",
        "\n",
        "\n",
        "    XTmodel1 = Sequential()\n",
        "    XTmodel1.add(Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128,  input_length=FLAGS.max_seq_len))\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)) #input_shape=(MAX_SEQ_LEN, SEQ_EMBEDDING_DIMS)\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1))\n",
        "    XTmodel1.add(Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1))\n",
        "    XTmodel1.add(GlobalMaxPooling1D())\n",
        "\n",
        "    interactionModel.add(layers.Concatenate([XDmodel, XTmodel1]))\n",
        "\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_single_protein.png')\n",
        "\n",
        "    return interactionModel\n"
      ],
      "metadata": {
        "id": "SRLLipRkfCVP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "    interactionModel = Sequential()\n",
        "\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.drug_count, )))\n",
        "\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    interactionModel.add(layers.Concatenate([XDmodel, XTmodel]))\n",
        "\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_baseline.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "UVRKjUvQfJvb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_baseline(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2):\n",
        "    interactionModel = Sequential()\n",
        "\n",
        "    XDmodel = Sequential()\n",
        "    XDmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.drug_count, )))\n",
        "\n",
        "    XTmodel = Sequential()\n",
        "    XTmodel.add(Dense(1, activation='linear', input_shape=(FLAGS.target_count,)))\n",
        "\n",
        "\n",
        "    interactionModel.add(layers.Concatenate([XDmodel, XTmodel]))\n",
        "\n",
        "    # Fully connected\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(1024, activation='relu'))\n",
        "    interactionModel.add(Dropout(0.1))\n",
        "    interactionModel.add(Dense(512, activation='relu'))\n",
        "\n",
        "    interactionModel.add(Dense(1, kernel_initializer='normal'))\n",
        "    interactionModel.compile(optimizer='adam', loss='mean_squared_error', metrics=[cindex_score])\n",
        "\n",
        "    print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_baseline.png')\n",
        "\n",
        "    return interactionModel"
      ],
      "metadata": {
        "id": "Q9PiUzBEfW8f"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nfold_1_2_3_setting_sample(XD, XT,  Y, label_row_inds, label_col_inds, measure, runmethod,  FLAGS, dataset):\n",
        "\n",
        "    bestparamlist = []\n",
        "    test_set, outer_train_sets = dataset.read_sets(FLAGS)\n",
        "\n",
        "    foldinds = len(outer_train_sets)\n",
        "\n",
        "    test_sets = []\n",
        "    ## TRAIN AND VAL\n",
        "    val_sets = []\n",
        "    train_sets = []\n",
        "\n",
        "    #logger.info('Start training')\n",
        "    for val_foldind in range(foldinds):\n",
        "        val_fold = outer_train_sets[val_foldind]\n",
        "        val_sets.append(val_fold)\n",
        "        otherfolds = deepcopy(outer_train_sets)\n",
        "        otherfolds.pop(val_foldind)\n",
        "        otherfoldsinds = [item for sublist in otherfolds for item in sublist]\n",
        "        train_sets.append(otherfoldsinds)\n",
        "        test_sets.append(test_set)\n",
        "        print(\"val set\", str(len(val_fold)))\n",
        "        print(\"train set\", str(len(otherfoldsinds)))\n",
        "    bestparamind, best_param_list, bestperf, all_predictions_not_need, losses_not_need = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds,\n",
        "                                                                                                measure, runmethod, FLAGS, train_sets, val_sets)\n",
        "    #print(\"Test Set len\", str(len(test_set)))\n",
        "    #print(\"Outer Train Set len\", str(len(outer_train_sets)))\n",
        "    bestparam, best_param_list, bestperf, all_predictions, all_losses = general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds,\n",
        "                                                                                                measure, runmethod, FLAGS, train_sets, test_sets)\n",
        "\n",
        "    testperf = all_predictions[bestparamind]##pointer pos\n",
        "\n",
        "    logging(\"---FINAL RESULTS-----\", FLAGS)\n",
        "    logging(\"best param index = %s,  best param = %.5f\" %\n",
        "            (bestparamind, bestparam), FLAGS)\n",
        "\n",
        "\n",
        "    testperfs = []\n",
        "    testloss= []\n",
        "\n",
        "    avgperf = 0.\n",
        "\n",
        "    for test_foldind in range(len(test_sets)):\n",
        "        foldperf = all_predictions[bestparamind][test_foldind]\n",
        "        foldloss = all_losses[bestparamind][test_foldind]\n",
        "        testperfs.append(foldperf)\n",
        "        testloss.append(foldloss)\n",
        "        avgperf += foldperf\n",
        "\n",
        "    avgperf = avgperf / len(test_sets)\n",
        "    avgloss = np.mean(testloss)\n",
        "    teststd = np.std(testperfs)\n",
        "\n",
        "    logging(\"Test Performance CI\", FLAGS)\n",
        "    logging(testperfs, FLAGS)\n",
        "    logging(\"Test Performance MSE\", FLAGS)\n",
        "    logging(testloss, FLAGS)\n",
        "\n",
        "    return avgperf, avgloss, teststd"
      ],
      "metadata": {
        "id": "eOz_h_yLfqiR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets): ## BURAYA DA FLAGS LAZIM????\n",
        "\n",
        "    paramset1 = FLAGS.num_windows                              #[32]#[32,  512] #[32, 128]  # filter numbers\n",
        "    paramset2 = FLAGS.smi_window_lengths                               #[4, 8]#[4,  32] #[4,  8] #filter length smi\n",
        "    paramset3 = FLAGS.seq_window_lengths                               #[8, 12]#[64,  256] #[64, 192]#[8, 192, 384]\n",
        "    epoch = FLAGS.num_epoch                                 #100\n",
        "    batchsz = FLAGS.batch_size                             #256\n",
        "\n",
        "    logging(\"---Parameter Search-----\", FLAGS)\n",
        "\n",
        "    w = len(val_sets)\n",
        "    h = len(paramset1) * len(paramset2) * len(paramset3)\n",
        "\n",
        "    all_predictions = [[0 for x in range(w)] for y in range(h)]\n",
        "    all_losses = [[0 for x in range(w)] for y in range(h)]\n",
        "    print(all_predictions)\n",
        "\n",
        "    for foldind in range(len(val_sets)):\n",
        "        valinds = val_sets[foldind]\n",
        "        labeledinds = labeled_sets[foldind]\n",
        "\n",
        "        Y_train = np.mat(np.copy(Y))\n",
        "\n",
        "        params = {}\n",
        "        XD_train = XD\n",
        "        XT_train = XT\n",
        "        trrows = label_row_inds[labeledinds]\n",
        "        trcols = label_col_inds[labeledinds]\n",
        "\n",
        "        XD_train = XD[trrows]\n",
        "        XT_train = XT[trcols]\n",
        "\n",
        "        train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, trrows, trcols)\n",
        "\n",
        "        terows = label_row_inds[valinds]\n",
        "        tecols = label_col_inds[valinds]\n",
        "        #print(\"terows\", str(terows), str(len(terows)))\n",
        "        #print(\"tecols\", str(tecols), str(len(tecols)))\n",
        "\n",
        "        val_drugs, val_prots,  val_Y = prepare_interaction_pairs(XD, XT,  Y, terows, tecols)\n",
        "\n",
        "\n",
        "        pointer = 0\n",
        "\n",
        "        for param1ind in range(len(paramset1)): #hidden neurons\n",
        "            param1value = paramset1[param1ind]\n",
        "            for param2ind in range(len(paramset2)): #learning rate\n",
        "                param2value = paramset2[param2ind]\n",
        "\n",
        "                for param3ind in range(len(paramset3)):\n",
        "                    param3value = paramset3[param3ind]\n",
        "\n",
        "                    gridmodel = runmethod(FLAGS, param1value, param2value, param3value)\n",
        "                    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "                    gridres = gridmodel.fit(([np.array(train_drugs),np.array(train_prots) ]), np.array(train_Y), batch_size=batchsz, epochs=epoch,\n",
        "                            validation_data=( ([np.array(val_drugs), np.array(val_prots) ]), np.array(val_Y)),  shuffle=False, callbacks=[es] )\n",
        "\n",
        "\n",
        "                    predicted_labels = gridmodel.predict([np.array(val_drugs), np.array(val_prots) ])\n",
        "                    loss, rperf2 = gridmodel.evaluate(([np.array(val_drugs),np.array(val_prots) ]), np.array(val_Y), verbose=0)\n",
        "                    rperf = prfmeasure(val_Y, predicted_labels)\n",
        "                    rperf = rperf[0]\n",
        "\n",
        "\n",
        "                    logging(\"P1 = %d,  P2 = %d, P3 = %d, Fold = %d, CI-i = %f, CI-ii = %f, MSE = %f\" %\n",
        "                    (param1ind, param2ind, param3ind, foldind, rperf, rperf2, loss), FLAGS)\n",
        "\n",
        "                    plotLoss(gridres, param1ind, param2ind, param3ind, foldind)\n",
        "\n",
        "                    all_predictions[pointer][foldind] =rperf #TODO FOR EACH VAL SET allpredictions[pointer][foldind]\n",
        "                    all_losses[pointer][foldind]= loss\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    bestperf = -float('Inf')\n",
        "    bestpointer = None\n",
        "\n",
        "\n",
        "    best_param_list = []\n",
        "    ##Take average according to folds, then chooose best params\n",
        "    pointer = 0\n",
        "    for param1ind in range(len(paramset1)):\n",
        "            for param2ind in range(len(paramset2)):\n",
        "                for param3ind in range(len(paramset3)):\n",
        "\n",
        "                    avgperf = 0.\n",
        "                    for foldind in range(len(val_sets)):\n",
        "                        foldperf = all_predictions[pointer][foldind]\n",
        "                        avgperf += foldperf\n",
        "                    avgperf /= len(val_sets)\n",
        "                    #print(epoch, batchsz, avgperf)\n",
        "                    if avgperf > bestperf:\n",
        "                        bestperf = avgperf\n",
        "                        bestpointer = pointer\n",
        "                        best_param_list = [param1ind, param2ind, param3ind]\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    return  bestpointer, best_param_list, bestperf, all_predictions, all_losses"
      ],
      "metadata": {
        "id": "P91sATmLfuSu"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cindex_score(y_true, y_pred):\n",
        "\n",
        "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
        "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
        "\n",
        "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
        "    f = tf.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
        "\n",
        "    g = tf.reduce_sum(tf.multiply(g, f))\n",
        "    f = tf.reduce_sum(f)\n",
        "\n",
        "    return tf.where(tf.equal(g, 0), 0.0, g/f) #select\n",
        "\n",
        "def plotLoss(history, batchind, epochind, param3ind, foldind):\n",
        "\n",
        "    figname = \"b\"+str(batchind) + \"_e\" + str(epochind) + \"_\" + str(param3ind) + \"_\"  + str( foldind) + \"_\" + str(time.time())\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\t#plt.legend(['trainloss', 'valloss', 'cindex', 'valcindex'], loc='upper left')\n",
        "    plt.legend(['trainloss', 'valloss'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname +\".png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait',\n",
        "                    papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "    ## PLOT CINDEX\n",
        "    plt.figure()\n",
        "    plt.title('model concordance index')\n",
        "    plt.ylabel('cindex')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.plot(history.history['cindex_score'])\n",
        "    plt.plot(history.history['val_cindex_score'])\n",
        "    plt.legend(['traincindex', 'valcindex'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname + \"_acc.png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait',\n",
        "                            papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "def prepare_interaction_pairs(XD, XT,  Y, rows, cols):\n",
        "    drugs = []\n",
        "    targets = []\n",
        "    targetscls = []\n",
        "    affinity=[]\n",
        "\n",
        "    for pair_ind in range(len(rows)):\n",
        "        drug = XD[rows[pair_ind]]\n",
        "        drugs.append(drug)\n",
        "\n",
        "        target=XT[cols[pair_ind]]\n",
        "        targets.append(target)\n",
        "\n",
        "        affinity.append(Y[rows[pair_ind],cols[pair_ind]])\n",
        "\n",
        "    drug_data = np.stack(drugs)\n",
        "    target_data = np.stack(targets)\n",
        "\n",
        "    return drug_data,target_data,affinity"
      ],
      "metadata": {
        "id": "Che6u4z1f3cq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment(FLAGS, perfmeasure, deepmethod, foldcount=6): #5-fold cross validation + test\n",
        "\n",
        "    #Input\n",
        "    #XD: [drugs, features] sized array (features may also be similarities with other drugs\n",
        "    #XT: [targets, features] sized array (features may also be similarities with other targets\n",
        "    #Y: interaction values, can be real values or binary (+1, -1), insert value float(\"nan\") for unknown entries\n",
        "    #perfmeasure: function that takes as input a list of correct and predicted outputs, and returns performance\n",
        "    #higher values should be better, so if using error measures use instead e.g. the inverse -error(Y, P)\n",
        "    #foldcount: number of cross-validation folds for settings 1-3, setting 4 always runs 3x3 cross-validation\n",
        "\n",
        "\n",
        "    dataset = DataSet( fpath = FLAGS.dataset_path, ### BUNU ARGS DA GUNCELLE\n",
        "                      setting_no = FLAGS.problem_type, ##BUNU ARGS A EKLE\n",
        "                      seqlen = FLAGS.max_seq_len,\n",
        "                      smilen = FLAGS.max_smi_len,\n",
        "                      need_shuffle = False )\n",
        "    # set character set size\n",
        "    FLAGS.charseqset_size = dataset.charseqset_size\n",
        "    FLAGS.charsmiset_size = dataset.charsmiset_size\n",
        "\n",
        "    XD, XT, Y = dataset.parse_data(FLAGS)\n",
        "\n",
        "    XD = np.asarray(XD)\n",
        "    XT = np.asarray(XT)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    drugcount = XD.shape[0]\n",
        "    print(drugcount)\n",
        "    targetcount = XT.shape[0]\n",
        "    print(targetcount)\n",
        "\n",
        "    FLAGS.drug_count = drugcount\n",
        "    FLAGS.target_count = targetcount\n",
        "\n",
        "    label_row_inds, label_col_inds = np.where(np.isnan(Y)==False)  #basically finds the point address of affinity [x,y]\n",
        "\n",
        "    if not os.path.exists(figdir):\n",
        "        os.makedirs(figdir)\n",
        "\n",
        "    print(FLAGS.log_dir)\n",
        "    S1_avgperf, S1_avgloss, S1_teststd = nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n",
        "                                                                     perfmeasure, deepmethod, FLAGS, dataset)\n",
        "\n",
        "    logging(\"Setting \" + str(FLAGS.problem_type), FLAGS)\n",
        "    logging(\"avg_perf = %.5f,  avg_mse = %.5f, std = %.5f\" %\n",
        "            (S1_avgperf, S1_avgloss, S1_teststd), FLAGS)"
      ],
      "metadata": {
        "id": "BI8FENWXf__0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_regression( FLAGS ):\n",
        "\n",
        "    perfmeasure = get_cindex\n",
        "    deepmethod = build_combined_categorical\n",
        "\n",
        "    experiment(FLAGS, perfmeasure, deepmethod)"
      ],
      "metadata": {
        "id": "l_eOEUTdgDsJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ArgumentParser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLH_UOQx-mrC",
        "outputId": "8adecc83-23af-478f-95bf-f16de1e9bd06"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ArgumentParser\n",
            "  Downloading argumentparser-1.2.1.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ArgumentParser\n",
            "  Building wheel for ArgumentParser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ArgumentParser: filename=argumentparser-1.2.1-py3-none-any.whl size=4849 sha256=575c25b3d09a458e8a52a866734b072bce02435abab9ae9bc9fbd47bef15e2c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/6f/58/0fa85fa0943152d1ea243cd4fea2ea6c25136f8972a8696df2\n",
            "Successfully built ArgumentParser\n",
            "Installing collected packages: ArgumentParser\n",
            "Successfully installed ArgumentParser-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from argparse import ArgumentParser\n",
        "import configparser"
      ],
      "metadata": {
        "id": "-pRy_BO19W6u"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    FLAGS = argparse.ArgumentParser()\n",
        "    FLAGS.log_dir = FLAGS.log_dir + str(time.time()) + \"/\"\n",
        "\n",
        "    if not os.path.exists(FLAGS.log_dir):\n",
        "        os.makedirs(FLAGS.log_dir)\n",
        "\n",
        "    logging(str(FLAGS), FLAGS)\n",
        "    run_regression( FLAGS )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "d6Dauy2KcbeP",
        "outputId": "e573095d-bd2b-479c-da39-7a3e04d37921"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-1acdafa4ff41>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ArgumentParser' object has no attribute 'log_dir'"
          ]
        }
      ]
    }
  ]
}