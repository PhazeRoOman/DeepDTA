{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhazeRoOman/DeepDTA/blob/master/Implementing_DeepDTA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iMWqMkwR46uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754ffc53-1cc5-40ab-b88e-bc160904a7c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1CqZypFa6jBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2432ce3-ef8d-4944-bb6a-bd20f0b7f493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PhazeRo/DeepDTA_Implementation/source\n",
            "arguments.py   go.sh                       new_rep_davis_backup.mat\n",
            "auc.jar        Implementing_DeepDTA.ipynb  new_rep_kiba_backup.mat\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/          indices_unique_davis.mat    \u001b[01;34m__pycache__\u001b[0m/\n",
            "datahelper.py  indices_unique_kiba.mat     run_experiments_original.py\n",
            "emetrics.py    \u001b[01;34mlogs\u001b[0m/                       run_experiments.py\n",
            "\u001b[01;34mfigures\u001b[0m/       model_deepDTA.keras\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/PhazeRo/DeepDTA_Implementation/source\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlkPV_qabCyU"
      },
      "source": [
        "/content/drive/MyDrive/PhazeRo/DeepDTA_Implementation/source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V97YSqY6fzs",
        "outputId": "07e20edd-a32c-4c44-ae25-dfea07cb7089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.10/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (1.23.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.2.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install matplotlib==3.2.2\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baba-saR6ev0",
        "outputId": "73a14769-2351-4c01-b076-4c69273ebd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: matplotlib\n",
            "Version: 3.2.2\n",
            "Summary: Python plotting package\n",
            "Home-page: https://matplotlib.org\n",
            "Author: John D. Hunter, Michael Droettboom\n",
            "Author-email: matplotlib-users@python.org\n",
            "License: PSF\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: cycler, kiwisolver, numpy, pyparsing, python-dateutil\n",
            "Required-by: arviz, datascience, fastai, flax, geemap, imgaug, matplotlib-venn, missingno, mizani, mlxtend, music21, plotnine, prophet, pycocotools, seaborn, wordcloud, yellowbrick\n"
          ]
        }
      ],
      "source": [
        "%pip show matplotlib\n",
        "# %pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mq3tY2jw6mJx"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "\n",
        "np.random.seed(1)\n",
        "rn.seed(1)\n",
        "\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "tf.compat.v1.keras.backend.set_session\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ox6k2ADDXP5r"
      },
      "outputs": [],
      "source": [
        "from datahelper import *\n",
        "from arguments import argparser, logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jV1WNLQm6mM2"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, GRU\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras import optimizers, layers\n",
        "from keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZYd98VL1XoLz"
      },
      "outputs": [],
      "source": [
        "import sys, pickle, os\n",
        "import math, json, time\n",
        "import decimal\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from random import shuffle\n",
        "from copy import deepcopy\n",
        "from sklearn import preprocessing\n",
        "from emetrics import get_aupr, get_cindex, get_rm2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OUuvBi3AXoCM"
      },
      "outputs": [],
      "source": [
        "TABSY = \"\\t\"\n",
        "figdir = \"figures/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OS8VI4Q7cRMN"
      },
      "outputs": [],
      "source": [
        "def build_combined_categorical(FLAGS, NUM_FILTERS, FILTER_LENGTH, learning_rate):\n",
        "\n",
        "    # FLAGS.num_windows = [32]  # num_filters\n",
        "    # FLAGS.seq_window_lengths = [4, 8, 12]#[8,12]  # kernel_size\n",
        "    # FLAGS.smi_window_lengths = [4]#[4,8]\n",
        "\n",
        "    XDinput = Input(shape=(FLAGS.max_smi_len,), dtype='int32') ### Buralar flagdan gelmeliii\n",
        "    XTinput = Input(shape=(FLAGS.max_seq_len,), dtype='int32')\n",
        "\n",
        "    print('NUM_FILTERS:', NUM_FILTERS)\n",
        "    print('FILTER_LENGTH:', FILTER_LENGTH)\n",
        "    print('learning_rate:', learning_rate)\n",
        "    ### SMI_EMB_DINMS  FLAGS GELMELII\n",
        "    encode_smiles = Embedding(input_dim=FLAGS.charsmiset_size+1, output_dim=128, input_length=FLAGS.max_smi_len)(XDinput)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH,  activation='relu', padding='valid',  strides=1)(encode_smiles)\n",
        "    encode_smiles = GlobalMaxPooling1D()(encode_smiles)\n",
        "\n",
        "    encode_protein = Embedding(input_dim=FLAGS.charseqset_size+1, output_dim=128, input_length=FLAGS.max_seq_len)(XTinput)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH,  activation='relu', padding='valid',  strides=1)(encode_protein)\n",
        "    encode_protein = GlobalMaxPooling1D()(encode_protein)\n",
        "\n",
        "    encode_interaction = keras.layers.concatenate([encode_smiles, encode_protein], axis=-1) #merge.Add()([encode_smiles, encode_protein])\n",
        "\n",
        "    # Fully connected\n",
        "    FC1 = Dense(1024, activation='relu')(encode_interaction)\n",
        "    FC2 = Dropout(0.1)(FC1)\n",
        "    FC2 = Dense(1024, activation='relu')(FC2)\n",
        "    FC2 = Dropout(0.1)(FC2)\n",
        "    FC2 = Dense(512, activation='relu')(FC2)\n",
        "\n",
        "    # And add a logistic regression on top\n",
        "    predictions = Dense(1, kernel_initializer='normal')(FC2) #OR no activation, rght now it's between 0-1, do I want this??? activation='sigmoid'\n",
        "\n",
        "    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])\n",
        "\n",
        "    adam=Adam(learning_rate=learning_rate)\n",
        "    interactionModel.compile(optimizer=adam, loss='mean_squared_error', metrics=[cindex_score]) #, metrics=['cindex_score']\n",
        "    # print(interactionModel.summary())\n",
        "    plot_model(interactionModel, to_file='figures/build_combined_categorical.png')\n",
        "\n",
        "    return interactionModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nfold_1_2_3_setting_sample(XD, XT,  Y, label_row_inds, label_col_inds, measure, runmethod,  FLAGS, dataset):\n",
        "\n",
        "    # bestparamlist = []\n",
        "    test_set, outer_train_sets = dataset.read_sets(FLAGS)\n",
        "\n",
        "    # print('\\n')\n",
        "    # print('*Printing in nfold_1_2_3_setting_sample*\\n')\n",
        "    # print('test_set:', test_set)\n",
        "    # print('max(test_set):', max(test_set))\n",
        "    # print('len(outer_train_sets):', len(outer_train_sets))\n",
        "    # print('max(max(outer_train_sets)):', max(max(outer_train_sets)))\n",
        "    # print('outer_train_sets:', outer_train_sets)\n",
        "\n",
        "    foldinds = len(outer_train_sets)\n",
        "\n",
        "    test_sets = []\n",
        "    ## TRAIN AND VAL\n",
        "    val_sets = []\n",
        "    train_sets = []\n",
        "\n",
        "    #logger.info('Start training')\n",
        "    for val_foldind in range(foldinds):\n",
        "        val_fold = outer_train_sets[val_foldind]\n",
        "        # print('val_foldind:', val_foldind)\n",
        "        # print('val_fold:', val_fold)\n",
        "        val_sets.append(val_fold)\n",
        "        otherfolds = deepcopy(outer_train_sets)\n",
        "        otherfolds.pop(val_foldind)\n",
        "        otherfoldsinds = [item for sublist in otherfolds for item in sublist]\n",
        "        train_sets.append(otherfoldsinds)\n",
        "        # test_sets.append(test_set)\n",
        "        print(\"val set\", str(len(val_fold)))\n",
        "        print(\"train set\", str(len(otherfoldsinds)))\n",
        "    test_sets.append(test_set)\n",
        "\n",
        "\n",
        "\n",
        "    # bestparamind, best_param_list, bestperf, all_predictions_not_need, \\\n",
        "    # losses_not_need, trained_model = general_nfold_cv(XD, XT,  Y, label_row_inds, \\\n",
        "    #                                                   label_col_inds, measure, \\\n",
        "    #                                                   runmethod, FLAGS, train_sets, \\\n",
        "    #                                                   val_sets)\n",
        "\n",
        "    best_parameters = []\n",
        "    bestparamind, best_param_list, bestperf, all_predictions_not_need, \\\n",
        "    losses_not_need, trained_model = general_nfold_cv(XD, XT,  Y,\n",
        "                                                      label_row_inds, label_col_inds,\n",
        "                                                      measure,\n",
        "                                                      runmethod,\n",
        "                                                      FLAGS,\n",
        "                                                      train_sets, val_sets,\n",
        "                                                      best_parameters,\n",
        "                                                      TUNING_FLAG=1,\n",
        "                                                      FITTING_FLAG=1)\n",
        "\n",
        "    bestparamind = 0\n",
        "    best_parameters = [FLAGS.num_windows[best_param_list[0]],\n",
        "                       FLAGS.smi_window_lengths[best_param_list[1]],\n",
        "                       FLAGS.seq_window_lengths[best_param_list[2]]]\n",
        "\n",
        "\n",
        "    #print(\"Test Set len\", str(len(test_set)))\n",
        "    #print(\"Outer Train Set len\", str(len(outer_train_sets)))\n",
        "    bestparam, best_param_list, bestperf, all_predictions, \\\n",
        "    all_losses, trained_model = general_nfold_cv(XD, XT, Y,\n",
        "                                                 label_row_inds, label_col_inds,\n",
        "                                                 measure,\n",
        "                                                 runmethod,\n",
        "                                                 FLAGS,\n",
        "                                                 train_sets, val_sets,\n",
        "                                                 best_parameters,\n",
        "                                                 TUNING_FLAG=0,\n",
        "                                                 FITTING_FLAG=1)\n",
        "\n",
        "    valperfs = []\n",
        "    valloss= []\n",
        "    avgperf = 0.\n",
        "\n",
        "    for val_foldind in range(len(val_sets)):\n",
        "        foldperf = all_predictions[bestparamind][val_foldind]\n",
        "        foldloss = all_losses[bestparamind][val_foldind]\n",
        "        valperfs.append(foldperf)\n",
        "        valloss.append(foldloss)\n",
        "        avgperf += foldperf\n",
        "\n",
        "    avgperf = avgperf / len(val_sets)\n",
        "    avgloss = np.mean(valloss)\n",
        "    valstd = np.std(valperfs)\n",
        "\n",
        "\n",
        "    bestparam, best_param_list, bestperf, all_predictions, \\\n",
        "    all_losses = general_nfold_cv(XD, XT, Y,\n",
        "                                  label_row_inds, label_col_inds,\n",
        "                                  measure,\n",
        "                                  trained_model,\n",
        "                                  FLAGS,\n",
        "                                  train_sets, test_sets,\n",
        "                                  best_parameters,\n",
        "                                  TUNING_FLAG=0,\n",
        "                                  FITTING_FLAG=0)\n",
        "\n",
        "\n",
        "    # print('all_predictions:', all_predictions)\n",
        "\n",
        "    logging(\"---FINAL RESULTS-----\", FLAGS)\n",
        "    logging(\"best param index = %s,  best param = %.5f\" %\n",
        "            (bestparamind, bestparam), FLAGS)\n",
        "\n",
        "    testperfs = bestperf\n",
        "    testloss = all_losses[0]\n",
        "\n",
        "    # testperfs = []\n",
        "    # testloss= []\n",
        "\n",
        "    # avgperf = 0.\n",
        "\n",
        "    # for test_foldind in range(len(test_sets)):\n",
        "    #   for test_foldind in range(len(test_set)):\n",
        "    #     foldperf = all_predictions[bestparamind][test_foldind]\n",
        "    #     foldloss = all_losses[bestparamind][test_foldind]\n",
        "    #     testperfs.append(foldperf)\n",
        "    #     testloss.append(foldloss)\n",
        "    #     avgperf += foldperf\n",
        "\n",
        "    # avgperf = avgperf / len(test_sets)\n",
        "    # avgloss = np.mean(testloss)\n",
        "    # teststd = np.std(testperfs)\n",
        "\n",
        "    logging(\"Test Performance CI\", FLAGS)\n",
        "    logging(testperfs, FLAGS)\n",
        "    logging(\"Test Performance MSE\", FLAGS)\n",
        "    logging(testloss, FLAGS)\n",
        "\n",
        "    # return avgperf, avgloss, teststd, trained_model\n",
        "    return avgperf, avgloss, valstd, trained_model"
      ],
      "metadata": {
        "id": "V0FXAkhmWBFM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def general_nfold_cv(XD, XT,  Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets, best_parameters, TUNING_FLAG, FITTING_FLAG): ## BURAYA DA FLAGS LAZIM????\n",
        "\n",
        "    if FITTING_FLAG != 1 and FITTING_FLAG != 0:\n",
        "      raise Exception('Error! FITTING_FLAG should be 1 or 0.')\n",
        "\n",
        "    if TUNING_FLAG == 1 and FITTING_FLAG == 0:\n",
        "      raise Exception('Error! TUNING_FLAG = 1 and FITTING_FLAG = 0. Are you sure?')\n",
        "\n",
        "    if TUNING_FLAG == 1:\n",
        "      paramset1 = FLAGS.num_windows                              #[32]#[32,  512] #[32, 128]  # filter numbers\n",
        "      paramset2 = FLAGS.smi_window_lengths                               #[4, 8]#[4,  32] #[4,  8] #filter length smi\n",
        "      paramset3 = FLAGS.seq_window_lengths                               #[8, 12]#[64,  256] #[64, 192]#[8, 192, 384]\n",
        "      PRINTING_FLAG = 'Tuning'\n",
        "    elif TUNING_FLAG == 0:\n",
        "      if FITTING_FLAG == 1:\n",
        "        PRINTING_FLAG = 'Final training'\n",
        "      elif FITTING_FLAG == 0:\n",
        "        PRINTING_FLAG = 'Evaluation only'\n",
        "      paramset1 = [best_parameters[0]]\n",
        "      paramset2 = [best_parameters[1]]\n",
        "      paramset3 = [best_parameters[2]]\n",
        "    else:\n",
        "      raise Exception('Error! Tuning flag should be 1 or 0.')\n",
        "\n",
        "    epoch = FLAGS.num_epoch                                 #100\n",
        "    batchsz = FLAGS.batch_size                             #256\n",
        "\n",
        "    logging(\"---Parameter Search-----\", FLAGS)\n",
        "\n",
        "    w = len(val_sets)\n",
        "    h = len(paramset1) * len(paramset2) * len(paramset3)\n",
        "\n",
        "    all_predictions = [[0 for x in range(w)] for y in range(h)]\n",
        "    all_losses = [[0 for x in range(w)] for y in range(h)]\n",
        "\n",
        "    # print('all_predictions:',all_predictions)\n",
        "\n",
        "    # print('len(val_sets):', len(val_sets))\n",
        "\n",
        "    print('\\n')\n",
        "    print('===========================================================')\n",
        "    print(PRINTING_FLAG)\n",
        "    print('Number of runs:', w*h)\n",
        "    print('Number of folds:', w)\n",
        "    print('Number of hyperparameters combinations for each fold:', h)\n",
        "    print('===========================================================')\n",
        "\n",
        "    runs_counter = 1\n",
        "\n",
        "    # if FITTING_FLAG == 0:\n",
        "    #   val_sets_length = 1\n",
        "    # else:\n",
        "    #   val_sets_length = len(val_sets)\n",
        "    for foldind in range(len(val_sets)):\n",
        "        # print()\n",
        "        # print('*Printing in general_nfold_cv*\\n')\n",
        "        # print('foldind:', foldind)\n",
        "        # print('val_sets:', val_sets)\n",
        "        valinds = val_sets[foldind]\n",
        "        # print('valinds = val_sets[foldind]:', valinds)\n",
        "        # print('labeled_sets:', labeled_sets)\n",
        "        labeledinds = labeled_sets[foldind]\n",
        "        # print('labeledinds = labeled_sets[foldind]:', labeledinds)\n",
        "\n",
        "        Y_train = np.mat(np.copy(Y))\n",
        "\n",
        "        params = {}\n",
        "        XD_train = XD\n",
        "        XT_train = XT\n",
        "        # print()\n",
        "        # print('labeledinds:', labeledinds)\n",
        "        # print('label_row_inds:', label_row_inds)\n",
        "        # print('label_col_inds:', label_col_inds)\n",
        "        # print()\n",
        "        trrows = label_row_inds[labeledinds]\n",
        "        trcols = label_col_inds[labeledinds]\n",
        "        # print()\n",
        "        # print('******* foldind:', foldind)\n",
        "        # print('len(trrows):', len(trrows))\n",
        "        # print('len(trcols):', len(trcols))\n",
        "        # print('trrows', trrows)\n",
        "        # print('trcols:', trcols)\n",
        "        # print('len(label_row_inds):', len(label_row_inds))\n",
        "        # print('len(label_col_inds):', len(label_col_inds))\n",
        "\n",
        "        XD_train = XD[trrows]\n",
        "        XT_train = XT[trcols]\n",
        "        # print()\n",
        "        # print('XD_train.shape:',XD_train.shape)\n",
        "        # print('XT_train.shape:',XD_train.shape)\n",
        "        # print()\n",
        "        # print('right before prepare_interaction_pairs for train:')\n",
        "        # print('XD.shape:', XD.shape)\n",
        "        # print('XT.shape:', XT.shape)\n",
        "        # print('Y.shape:', Y.shape)\n",
        "\n",
        "        train_drugs, train_prots,  train_Y = prepare_interaction_pairs(XD, XT, Y, trrows, trcols)\n",
        "\n",
        "        # print()\n",
        "        # print('train_drugs.shape:',train_drugs.shape, type(train_drugs))\n",
        "        # print('train_prots.shape:',train_prots.shape, type(train_prots))\n",
        "        # print('len(train_Y):', len(train_Y), type(train_Y))\n",
        "\n",
        "        terows = label_row_inds[valinds]\n",
        "        tecols = label_col_inds[valinds]\n",
        "        # print()\n",
        "        # print('len(terows):', len(terows))\n",
        "        # print('len(tecols):', len(tecols))\n",
        "        # print('terows', terows)\n",
        "        # print('tecols:', tecols)\n",
        "        # print('len(label_row_inds):', len(label_row_inds))\n",
        "        # print('len(label_col_inds):', len(label_col_inds))\n",
        "        #print(\"terows\", str(terows), str(len(terows)))\n",
        "        #print(\"tecols\", str(tecols), str(len(tecols)))\n",
        "\n",
        "        val_drugs, val_prots,  val_Y = prepare_interaction_pairs(XD, XT,  Y, terows, tecols)\n",
        "\n",
        "        # print()\n",
        "        # print('val_drugs.shape:', val_drugs.shape, type(val_drugs))\n",
        "        # print('val_prots.shape:', val_prots.shape, type(val_prots))\n",
        "        # print('len(val_Y):', len(val_Y), type(val_Y))\n",
        "        # print()\n",
        "\n",
        "        pointer = 0\n",
        "        # print('len(paramset1):',len(paramset1))\n",
        "        # print('len(paramset2):',len(paramset2))\n",
        "        # print('len(paramset3):',len(paramset3))\n",
        "        for param1ind in range(len(paramset1)): #hidden neurons\n",
        "            param1value = paramset1[param1ind]\n",
        "            for param2ind in range(len(paramset2)): #learning rate\n",
        "                param2value = paramset2[param2ind]\n",
        "                for param3ind in range(len(paramset3)):\n",
        "                    param3value = paramset3[param3ind]\n",
        "\n",
        "                    # deepmethod = build_combined_categorical\n",
        "                    # build_combined_categorical(FLAGS, NUM_FILTERS, FILTER_LENGTH1, FILTER_LENGTH2)\n",
        "                    # paramset1 = FLAGS.num_windows                              #[32]#[32,  512] #[32, 128]  # filter numbers\n",
        "                    # paramset2 = FLAGS.smi_window_lengths                               #[4, 8]#[4,  32] #[4,  8] #filter length smi\n",
        "                    # paramset3 = FLAGS.seq_window_lengths                               #[8, 12]#[64,  256] #[64, 192]#[8, 192, 384]\n",
        "                    print()\n",
        "                    print(f'runs_counter: {runs_counter}/{w*h} --> {PRINTING_FLAG}.')\n",
        "                    runs_counter += 1\n",
        "                    print('fold #:', foldind+1)\n",
        "                    if FITTING_FLAG == 1:\n",
        "                      gridmodel = runmethod(FLAGS, param1value, param2value, param3value)\n",
        "                      es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "                      gridres = gridmodel.fit(([np.array(train_drugs),np.array(train_prots) ]), np.array(train_Y), batch_size=batchsz, epochs=epoch,\n",
        "                                              validation_data=( ([np.array(val_drugs), np.array(val_prots) ]), np.array(val_Y)),  shuffle=False, callbacks=[es] )\n",
        "\n",
        "                    elif FITTING_FLAG == 0:\n",
        "                      gridmodel = runmethod\n",
        "\n",
        "                    predicted_labels = gridmodel.predict([np.array(val_drugs), np.array(val_prots) ])\n",
        "                    loss, rperf2 = gridmodel.evaluate(([np.array(val_drugs),np.array(val_prots) ]), np.array(val_Y), verbose=0)\n",
        "                    rperf = prfmeasure(val_Y, predicted_labels)\n",
        "                    rperf = rperf[0]\n",
        "\n",
        "                    logging(\"P1 = %d,  P2 = %d, P3 = %d, Fold = %d, CI-i = %f, CI-ii = %f, MSE = %f\" %\n",
        "                    (param1ind, param2ind, param3ind, foldind, rperf, rperf2, loss), FLAGS)\n",
        "\n",
        "                    if FITTING_FLAG == 1:\n",
        "                      plotLoss(gridres, param1ind, param2ind, param3ind, foldind, TUNING_FLAG)\n",
        "\n",
        "                    all_predictions[pointer][foldind] = rperf #TODO FOR EACH VAL SET allpredictions[pointer][foldind]\n",
        "                    all_losses[pointer][foldind]= loss\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    bestperf = -float('Inf')\n",
        "    bestpointer = None\n",
        "\n",
        "\n",
        "    best_param_list = []\n",
        "    ##Take average according to folds, then chooose best params\n",
        "    pointer = 0\n",
        "    for param1ind in range(len(paramset1)):\n",
        "            for param2ind in range(len(paramset2)):\n",
        "                for param3ind in range(len(paramset3)):\n",
        "                    avgperf = 0.\n",
        "                    for foldind in range(len(val_sets)):\n",
        "                        foldperf = all_predictions[pointer][foldind]\n",
        "                        avgperf += foldperf\n",
        "                    avgperf /= len(val_sets)\n",
        "                    #print(epoch, batchsz, avgperf)\n",
        "                    if avgperf > bestperf:\n",
        "                        bestperf = avgperf\n",
        "                        bestpointer = pointer\n",
        "                        best_param_list = [param1ind, param2ind, param3ind]\n",
        "\n",
        "                    pointer +=1\n",
        "\n",
        "    if FITTING_FLAG == 1:\n",
        "      return  bestpointer, best_param_list, bestperf, all_predictions, all_losses, gridmodel\n",
        "    elif FITTING_FLAG == 0:\n",
        "      return  bestpointer, best_param_list, bestperf, all_predictions, all_losses"
      ],
      "metadata": {
        "id": "bh7nlpZJWGKn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5RKTPBz9cuTy"
      },
      "outputs": [],
      "source": [
        "def cindex_score(y_true, y_pred):\n",
        "\n",
        "    g = tf.subtract(tf.expand_dims(y_pred, -1), y_pred)\n",
        "    g = tf.cast(g == 0.0, tf.float32) * 0.5 + tf.cast(g > 0.0, tf.float32)\n",
        "\n",
        "    f = tf.subtract(tf.expand_dims(y_true, -1), y_true) > 0.0\n",
        "    f = tf.compat.v1.matrix_band_part(tf.cast(f, tf.float32), -1, 0)\n",
        "\n",
        "    g = tf.reduce_sum(tf.multiply(g, f))\n",
        "    f = tf.reduce_sum(f)\n",
        "\n",
        "    return tf.where(tf.equal(g, 0), 0.0, g/f) #select"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotLoss(history, batchind, epochind, param3ind, foldind, TUNING_FLAG):\n",
        "\n",
        "    if TUNING_FLAG:\n",
        "      figname = \"b\"+str(batchind) + \"_e\" + str(epochind) + \"_\" + str(param3ind) + \"_\"  + str( foldind) + \"_\" + str(time.time())\n",
        "    else:\n",
        "      figname = \"FINAL_TUNED_b\"+str(batchind) + \"_e\" + str(epochind) + \"_\" + str(param3ind) + \"_\"  + str(foldind) + \"_\" + str(time.time())\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "\t#plt.legend(['trainloss', 'valloss', 'cindex', 'valcindex'], loc='upper left')\n",
        "    plt.legend(['trainloss', 'valloss'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname +\".png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait',\n",
        "                    papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1)#,frameon=None)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    ## PLOT CINDEX\n",
        "    plt.figure()\n",
        "    plt.title('model concordance index')\n",
        "    plt.ylabel('cindex')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.plot(history.history['cindex_score'])\n",
        "    plt.plot(history.history['val_cindex_score'])\n",
        "    plt.legend(['traincindex', 'valcindex'], loc='upper left')\n",
        "    plt.savefig(\"figures/\"+figname + \"_acc.png\" , dpi=None, facecolor='w', edgecolor='w', orientation='portrait',\n",
        "                            papertype=None, format=None,transparent=False, bbox_inches=None, pad_inches=0.1)#,frameon=None)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "K1i2oVErWMKt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k6woDI0ocSzm"
      },
      "outputs": [],
      "source": [
        "def prepare_interaction_pairs(XD, XT,  Y, rows, cols):\n",
        "    drugs = []\n",
        "    targets = []\n",
        "    targetscls = []\n",
        "    affinity=[]\n",
        "\n",
        "    for pair_ind in range(len(rows)):\n",
        "        drug = XD[rows[pair_ind]]\n",
        "        drugs.append(drug)\n",
        "\n",
        "        target=XT[cols[pair_ind]]\n",
        "        targets.append(target)\n",
        "\n",
        "        affinity.append(Y[rows[pair_ind],cols[pair_ind]])\n",
        "\n",
        "    drug_data = np.stack(drugs)\n",
        "    target_data = np.stack(targets)\n",
        "\n",
        "    return drug_data, target_data, affinity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PoFYZXIxcniE"
      },
      "outputs": [],
      "source": [
        "def experiment(FLAGS, perfmeasure, deepmethod, foldcount=6): #5-fold cross validation + test\n",
        "\n",
        "    #Input\n",
        "    #XD: [drugs, features] sized array (features may also be similarities with other drugs\n",
        "    #XT: [targets, features] sized array (features may also be similarities with other targets\n",
        "    #Y: interaction values, can be real values or binary (+1, -1), insert value float(\"nan\") for unknown entries\n",
        "    #perfmeasure: function that takes as input a list of correct and predicted outputs, and returns performance\n",
        "    #higher values should be better, so if using error measures use instead e.g. the inverse -error(Y, P)\n",
        "    #foldcount: number of cross-validation folds for settings 1-3, setting 4 always runs 3x3 cross-validation\n",
        "\n",
        "\n",
        "    dataset = DataSet( fpath = FLAGS.dataset_path, ### BUNU ARGS DA GUNCELLE\n",
        "                      setting_no = FLAGS.problem_type, ##BUNU ARGS A EKLE\n",
        "                      seqlen = FLAGS.max_seq_len,\n",
        "                      smilen = FLAGS.max_smi_len,\n",
        "                      need_shuffle = False )\n",
        "    # set character set size\n",
        "    FLAGS.charseqset_size = dataset.charseqset_size\n",
        "    FLAGS.charsmiset_size = dataset.charsmiset_size\n",
        "\n",
        "    XD, XT, Y = dataset.parse_data(FLAGS)\n",
        "\n",
        "    XD = np.asarray(XD)\n",
        "    XT = np.asarray(XT)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    drugcount = XD.shape[0]\n",
        "    print(drugcount)\n",
        "    targetcount = XT.shape[0]\n",
        "    print(targetcount)\n",
        "\n",
        "    FLAGS.drug_count = drugcount\n",
        "    FLAGS.target_count = targetcount\n",
        "\n",
        "    label_row_inds, label_col_inds = np.where(np.isnan(Y)==False)  #basically finds the point address of affinity [x,y]\n",
        "\n",
        "    if not os.path.exists(figdir):\n",
        "        os.makedirs(figdir)\n",
        "\n",
        "    print(FLAGS.log_dir)\n",
        "    S1_avgperf, S1_avgloss, S1_teststd, trained_model = nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n",
        "                                                                     perfmeasure, deepmethod, FLAGS, dataset)\n",
        "\n",
        "    logging(\"Setting \" + str(FLAGS.problem_type), FLAGS)\n",
        "    logging(\"avg_perf = %.5f,  avg_mse = %.5f, std = %.5f\" %\n",
        "            (S1_avgperf, S1_avgloss, S1_teststd), FLAGS)\n",
        "\n",
        "    return trained_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X1v8eF0Icne8"
      },
      "outputs": [],
      "source": [
        "def run_regression( FLAGS ):\n",
        "\n",
        "    perfmeasure = get_cindex\n",
        "    deepmethod = build_combined_categorical\n",
        "\n",
        "    trained_model = experiment(FLAGS, perfmeasure, deepmethod)\n",
        "\n",
        "    return trained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JxrHCKYarQV"
      },
      "source": [
        "/content/drive/MyDrive/PhazeRo/DeepDTA_Implementation/source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1Z6e28Qic_2p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c50b67ff-7a87-499b-d092-2e96aa139a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read data/davis/ start\n",
            "68\n",
            "442\n",
            "logs/1695823136.1194086/\n",
            "Reading data/davis/ start\n",
            "val set 5010\n",
            "train set 20036\n",
            "val set 5009\n",
            "train set 20037\n",
            "val set 5009\n",
            "train set 20037\n",
            "val set 5009\n",
            "train set 20037\n",
            "val set 5009\n",
            "train set 20037\n",
            "\n",
            "\n",
            "===========================================================\n",
            "Tuning\n",
            "Number of runs: 30\n",
            "Number of folds: 5\n",
            "Number of hyperparameters combinations for each fold: 6\n",
            "===========================================================\n",
            "\n",
            "runs_counter: 1/30 --> Tuning.\n",
            "fold #: 1\n",
            "NUM_FILTERS: 32\n",
            "FILTER_LENGTH: 4\n",
            "learning_rate: 0.001\n",
            "79/79 [==============================] - 22s 262ms/step - loss: 2.6720 - cindex_score: 0.6350 - val_loss: 0.6512 - val_cindex_score: 0.7351\n",
            "157/157 [==============================] - 1s 8ms/step\n",
            "\n",
            "runs_counter: 2/30 --> Tuning.\n",
            "fold #: 1\n",
            "NUM_FILTERS: 32\n",
            "FILTER_LENGTH: 4\n",
            "learning_rate: 0.0001\n",
            "79/79 [==============================] - 21s 253ms/step - loss: 10.2297 - cindex_score: 0.5456 - val_loss: 0.7699 - val_cindex_score: 0.6306\n",
            "157/157 [==============================] - 1s 8ms/step\n",
            "\n",
            "runs_counter: 3/30 --> Tuning.\n",
            "fold #: 1\n",
            "NUM_FILTERS: 32\n",
            "FILTER_LENGTH: 8\n",
            "learning_rate: 0.001\n",
            "79/79 [==============================] - 37s 456ms/step - loss: 3.3742 - cindex_score: 0.5952 - val_loss: 0.6768 - val_cindex_score: 0.7173\n",
            "157/157 [==============================] - 2s 12ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ddfe5b4d6ee0>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_regression\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_deepDTA.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-76a99a4b7561>\u001b[0m in \u001b[0;36mrun_regression\u001b[0;34m(FLAGS)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdeepmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_combined_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperfmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-41bf8aa0e013>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(FLAGS, perfmeasure, deepmethod, foldcount)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     S1_avgperf, S1_avgloss, S1_teststd, trained_model = nfold_1_2_3_setting_sample(XD, XT, Y, label_row_inds, label_col_inds,\n\u001b[0m\u001b[1;32m     42\u001b[0m                                                                      perfmeasure, deepmethod, FLAGS, dataset)\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2fd0cd2428fb>\u001b[0m in \u001b[0;36mnfold_1_2_3_setting_sample\u001b[0;34m(XD, XT, Y, label_row_inds, label_col_inds, measure, runmethod, FLAGS, dataset)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mbestparamind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_param_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestperf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions_not_need\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     losses_not_need, trained_model = general_nfold_cv(XD, XT,  Y,\n\u001b[0m\u001b[1;32m     47\u001b[0m                                                       \u001b[0mlabel_row_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col_inds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                       \u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e881748433a9>\u001b[0m in \u001b[0;36mgeneral_nfold_cv\u001b[0;34m(XD, XT, Y, label_row_inds, label_col_inds, prfmeasure, runmethod, FLAGS, labeled_sets, val_sets, best_parameters, TUNING_FLAG, FITTING_FLAG)\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_drugs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_prots\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrperf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgridmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_drugs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_prots\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mrperf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprfmeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0mrperf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrperf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/PhazeRo/DeepDTA_Implementation/source/emetrics.py\u001b[0m in \u001b[0;36mget_cindex\u001b[0;34m(Y, P)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_aupr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# if __name__==\"__main__\":\n",
        "\n",
        "# hyper_params_dict = {'kernel_size': [4, 8, 12],\n",
        "#                      'num_filters': [32],\n",
        "#                      'learning_rate': [0.001, 0.0001]}\n",
        "\n",
        "# paramset1 = FLAGS.num_windows\n",
        "# paramset2 = FLAGS.smi_window_lengths\n",
        "# paramset3 = FLAGS.seq_window_lengths\n",
        "\n",
        "FLAGS = argparser()\n",
        "\n",
        "FLAGS.num_windows = [32]  # num_filters\n",
        "FLAGS.smi_window_lengths = [4, 8, 12] #[4,8]  # kernel_size\n",
        "FLAGS.seq_window_lengths = [0.001, 0.0001] #[8,12]  # will make this the learning_rate just for comparison with the new arch.\n",
        "\n",
        "FLAGS.batch_size = 256\n",
        "FLAGS.num_epoch = 1\n",
        "FLAGS.max_seq_len = 1000\n",
        "FLAGS.max_smi_len = 100\n",
        "FLAGS.dataset_path = 'data/davis/'\n",
        "FLAGS.problem_type = 1\n",
        "FLAGS.log_dir = 'logs/' + str(time.time()) + \"/\"\n",
        "# check for the is_log FLAG\n",
        "dataset_name = FLAGS.dataset_path.split('/')[1]\n",
        "if dataset_name == 'davis':\n",
        "  FLAGS.is_log = 1\n",
        "else:\n",
        "  FLAGS.is_log = 0\n",
        "\n",
        "\n",
        "if not os.path.exists(FLAGS.log_dir):\n",
        "  os.makedirs(FLAGS.log_dir)\n",
        "\n",
        "logging(str(FLAGS), FLAGS)\n",
        "trained_model = run_regression( FLAGS )\n",
        "trained_model.save('model_deepDTA.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz9LRdOi8z1g"
      },
      "outputs": [],
      "source": [
        "trained_model.save('model_deepDTA.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zesQSJVMblIi"
      },
      "outputs": [],
      "source": [
        "reconstructed_model = keras.models.load_model(\"model_deepDTA.keras\", custom_objects={\"cindex_score\": cindex_score})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zltoOWtc0guM"
      },
      "outputs": [],
      "source": [
        "reconstructed_model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}